Week 3 Notes

Optimization: choosing the best option from a set of option

- Local search
search algorithm that maintain a single node and searches by moving to a neighboring node
> state-space landscape: finding global maximum or global minimum

- Hill climbing algorithm (steepest-ascent)
> look neighbour, try to pick the higher/lower values
> pseudocode:
function Hill-Climbing(Problem):
	current = initial state of problem
	repeat:
		neighbor = highest valued neighbor of current (when maximize)
		if neighbor not better than current:
			return current
		current = neighbor
> limitation: might not always give the optimal solution (stuck in local maxima/minima)
> limitation: flat local maximum/minimum (might get stuck); shoulder (flat area)
> types of hill-climbing:
steepest-ascent: choose the highest-valued neighbor
stochastic: choose randomly from higher-valued neighbors
first-choice: choose the first higher-valued neighbor
random-restart: conduct hill climbing multiple times
local beam search: chooses the k highest-valued neighbors
*see source code for real example

- Simulated Annealing
> Early on, higher 'temperature': more likely to accept neighbors that are worse than current state
> Later on, lower 'temperature': less likely to accept neighbors that are worse than current state
> pseudocode:
function SIMULATED-ANNEALING(problem, max):
	current = initial state of problem
	for t=1 to max:
		T = TEMPERATURE(t)
		neighbor = random neighbor of current
		deltaE = how much better is than current
			if deltaE > 0:
				current = neighbor
			with probability exp(deltaE/T) set current = neighbor
		return current
*travelling salesman problem

- Linear Programming
> trying to optimize mathematical values (considering cost function and constraint)
> ex: trying to minimize cost function c1x1+c2x2+...+cnxn
with constraints of form a1x1+a2x2+...+anxn<=b or =b or bounds for each variable
> simplex algorithm: 
> interior-point algorithm: 
*using scipy module for linear programming

- Constraint Satisfaction Problem
*graph theory
> Set of variables (X1, X2, ..., Xn)
> Set of domains (D1, D2, ..., Dn)
> Set of constraints C (vary); hard constraints / soft constraints;
unary constraint (single variable), binary constraint (ex: A=/=B);
*ex: sudoku game
> node consistency: when all values in a variable's domain satisfy the unary constraints
> arc consistency: --/-- satisfy binary constraint
In order to make X arc-consistent with respect to Y, remove elements from X's domain until every
choice of X has a possible choice for Y
> pseudocode (enforce for a single arc):
function REVISE(csp, X, Y):
	revised = False
	for x in X.domain:
		if no y in Y.domain satisfies constraint for (X, Y):
			delete x from X.domain
			revised = true
	return revised
> pseudocode (enforce arc consistency for entire problem):
function AC-3(csp):
	queue = all arcs in csp
	while queue non-empty:
		(X, Y) = DEQUEUE(queue)
		if REVISE(csp, X, Y):
			if size of X.domain == 0:
				return False
			for each Z in X.neighbors - {Y}:
				enqueue(queue, (Z, X))
	return True
*recall search problem (CSPs as Search Problems)

- Backtracking Search
> backtrack anytime stuck with the csp
> pseudocode:
function BACKTRACK(assignment, csp):
	if assignment complete: return assignment
	var = SELECT-UNASSIGNED-VAR(assignment, csp)
	for value in DOMAIN-VALUES(var, assignment, csp):
		if value consistent with assignment:
			add {var=value} to assignment
			result = BACKTRACK(assignment, csp)
			if result =/= failure: return result
		remove {var=value} from assignment
	return failure
*example in source code
*example from constraint library
> using inference to solve csf: making sure arc consistency
maintaining arc-consistency: algorithm for enforcing arc-consistency every new assignment
> SELECT-UNASSIGNED-VAR function: choosing carefully which variable to explore next for efficiency:
minimum remaining values (MRV) heuristic: select variable that has the smallest domain
degree heuristic: select the variable that has the highest degree
> DOMAIN_VALUES:
least constraining values heuristic: return variables in order by number of choices that are ruled
out for neighboring variables; try least-constraining values first